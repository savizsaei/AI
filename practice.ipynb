{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4db8d794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from faiss-cpu) (23.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Converting documents to embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FAISS index with vector dimension: 384\n",
      "\n",
      "--- Testing Semantic Search ---\n",
      "\n",
      "Query: 'How do vector databases work?'\n",
      "  1. Vector databases store high-dimensional vectors for similarity search. (distance: 0.7787)\n",
      "  2. Vector search enables semantic search capabilities. (distance: 0.9240)\n",
      "  3. Weaviate is a vector search engine with GraphQL API. (distance: 1.1274)\n",
      "\n",
      "Query: 'What is artificial intelligence?'\n",
      "  1. Machine learning is a subfield of artificial intelligence. (distance: 0.7114)\n",
      "  2. Azure AI Search provides cognitive search capabilities. (distance: 1.1847)\n",
      "  3. Neural networks have transformed natural language processing. (distance: 1.4932)\n",
      "\n",
      "Query: 'Tell me about similarity search'\n",
      "  1. Vector databases store high-dimensional vectors for similarity search. (distance: 0.6221)\n",
      "  2. FAISS is an efficient similarity search library for dense vectors. (distance: 0.7312)\n",
      "  3. Semantic search understands the intent behind user queries. (distance: 0.9937)\n",
      "\n",
      "--- Simple Evaluation ---\n",
      "                                                             query  \\\n",
      "How do vector databases work?        How do vector databases work?   \n",
      "What is artificial intelligence?  What is artificial intelligence?   \n",
      "Tell me about similarity search    Tell me about similarity search   \n",
      "\n",
      "                                  Machine learning is a subfield of artificial intelligence.  \\\n",
      "How do vector databases work?                                              1.218511            \n",
      "What is artificial intelligence?                                           0.843465            \n",
      "Tell me about similarity search                                            1.219842            \n",
      "\n",
      "                                  Vector databases store high-dimensional vectors for similarity search.  \\\n",
      "How do vector databases work?                                              0.882450                        \n",
      "What is artificial intelligence?                                           1.341289                        \n",
      "Tell me about similarity search                                            0.788729                        \n",
      "\n",
      "                                  FAISS is an efficient similarity search library for dense vectors.  \\\n",
      "How do vector databases work?                                              1.116309                    \n",
      "What is artificial intelligence?                                           1.380057                    \n",
      "Tell me about similarity search                                            0.855095                    \n",
      "\n",
      "                                  Neural networks have transformed natural language processing.  \\\n",
      "How do vector databases work?                                              1.296116               \n",
      "What is artificial intelligence?                                           1.221966               \n",
      "Tell me about similarity search                                            1.329928               \n",
      "\n",
      "                                  Embeddings capture semantic meaning in a vector space.  \\\n",
      "How do vector databases work?                                              1.134295        \n",
      "What is artificial intelligence?                                           1.268054        \n",
      "Tell me about similarity search                                            1.239543        \n",
      "\n",
      "                                  Pinecone is a managed vector database service.  \\\n",
      "How do vector databases work?                                           1.080899   \n",
      "What is artificial intelligence?                                        1.330912   \n",
      "Tell me about similarity search                                         1.269714   \n",
      "\n",
      "                                  Vector search enables semantic search capabilities.  \\\n",
      "How do vector databases work?                                              0.961242     \n",
      "What is artificial intelligence?                                           1.237022     \n",
      "Tell me about similarity search                                            1.039275     \n",
      "\n",
      "                                  Weaviate is a vector search engine with GraphQL API.  \\\n",
      "How do vector databases work?                                              1.061806      \n",
      "What is artificial intelligence?                                           1.257146      \n",
      "Tell me about similarity search                                            1.091054      \n",
      "\n",
      "                                  Azure AI Search provides cognitive search capabilities.  \\\n",
      "How do vector databases work?                                              1.302320         \n",
      "What is artificial intelligence?                                           1.088431         \n",
      "Tell me about similarity search                                            1.164520         \n",
      "\n",
      "                                  Semantic search understands the intent behind user queries.  \n",
      "How do vector databases work?                                              1.124635            \n",
      "What is artificial intelligence?                                           1.239647            \n",
      "Tell me about similarity search                                            0.996850            \n",
      "\n",
      "This evaluation shows the distance between each query and all documents.\n",
      "Lower distances indicate better semantic matches.\n"
     ]
    }
   ],
   "source": [
    "# Semantic Search with FAISS - Practice Example\n",
    "import numpy as np\n",
    "%pip install faiss-cpu\n",
    "%pip install torch\n",
    "%pip install sentence-transformers\n",
    "import faiss\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Set up your data\n",
    "# We'll create a small collection of documents to search through\n",
    "documents = [\n",
    "    \"Machine learning is a subfield of artificial intelligence.\",\n",
    "    \"Vector databases store high-dimensional vectors for similarity search.\",\n",
    "    \"FAISS is an efficient similarity search library for dense vectors.\",\n",
    "    \"Neural networks have transformed natural language processing.\",\n",
    "    \"Embeddings capture semantic meaning in a vector space.\",\n",
    "    \"Pinecone is a managed vector database service.\",\n",
    "    \"Vector search enables semantic search capabilities.\",\n",
    "    \"Weaviate is a vector search engine with GraphQL API.\",\n",
    "    \"Azure AI Search provides cognitive search capabilities.\",\n",
    "    \"Semantic search understands the intent behind user queries.\"\n",
    "]\n",
    "\n",
    "# Step 2: Install and import libraries\n",
    "# You would typically run: pip install faiss-cpu torch sentence-transformers pandas\n",
    "# (Use faiss-gpu instead if you have GPU support)\n",
    "\n",
    "# Step 3: Convert documents to embeddings\n",
    "print(\"Converting documents to embeddings...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', cache_folder='./model_cache')  # A small, fast model\n",
    "embeddings = model.encode(documents)\n",
    "vector_dimension = embeddings.shape[1]  # Get the embedding dimension\n",
    "\n",
    "# Step 4: Build the FAISS index\n",
    "print(f\"Building FAISS index with vector dimension: {vector_dimension}\")\n",
    "index = faiss.IndexFlatL2(vector_dimension)  # L2 distance (Euclidean)\n",
    "# Convert embeddings to float32 (required by FAISS)\n",
    "embeddings = embeddings.astype(np.float32)\n",
    "index.add(embeddings)  # Add vectors to the index\n",
    "\n",
    "# Step 5: Perform a search\n",
    "def search(query_text, top_k=3):\n",
    "    # Convert query to embedding\n",
    "    query_vector = model.encode([query_text])[0].astype(np.float32)\n",
    "    query_vector = np.array([query_vector])  # Reshape for FAISS\n",
    "    \n",
    "    # Search the index\n",
    "    distances, indices = index.search(query_vector, top_k)\n",
    "    \n",
    "    # Return results\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx != -1:  # Valid index\n",
    "            results.append({\n",
    "                \"document\": documents[idx],\n",
    "                \"distance\": distances[0][i]\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Step 6: Test searches\n",
    "print(\"\\n--- Testing Semantic Search ---\")\n",
    "test_queries = [\n",
    "    \"How do vector databases work?\",\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Tell me about similarity search\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    results = search(query)\n",
    "    for i, res in enumerate(results):\n",
    "        print(f\"  {i+1}. {res['document']} (distance: {res['distance']:.4f})\")\n",
    "\n",
    "# Step 7: Bonus - Simple evaluation\n",
    "print(\"\\n--- Simple Evaluation ---\")\n",
    "# Create a dataframe to compare distances\n",
    "eval_df = pd.DataFrame(columns=['query'] + documents)\n",
    "\n",
    "for query in test_queries:\n",
    "    query_vector = model.encode([query])[0].astype(np.float32)\n",
    "    distances = []\n",
    "    for doc in documents:\n",
    "        doc_vector = model.encode([doc])[0].astype(np.float32)\n",
    "        # Calculate L2 distance\n",
    "        distance = np.linalg.norm(query_vector - doc_vector)\n",
    "        distances.append(distance)\n",
    "    \n",
    "    eval_df.loc[query] = [query] + distances\n",
    "\n",
    "print(eval_df.head())\n",
    "print(\"\\nThis evaluation shows the distance between each query and all documents.\")\n",
    "print(\"Lower distances indicate better semantic matches.\")\n",
    "\n",
    "\n",
    "# How to run this example:\n",
    "# 1. Save this code to a file (e.g., faiss_practice.py)\n",
    "# 2. Install the required packages: pip install faiss-cpu torch sentence-transformers pandas\n",
    "# 3. Run the script: python faiss_practice.py\n",
    "# 4. Experiment with different queries, models, or distance metrics\n",
    "\n",
    "# Extensions to try:\n",
    "# - Add more documents or load real data from a CSV/JSON file\n",
    "# - Try different embedding models\n",
    "# - Implement other FAISS index types like IndexIVFFlat for larger datasets\n",
    "# - Add metadata to your documents and return it with search results\n",
    "# - Compare results with other vector databases like Pinecone or Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84f0484e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context 'Dummy context', the answer to 'How do vector databases work?' is...\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import Dict, Any, TypedDict\n",
    "\n",
    "# Define the state schema\n",
    "class GraphState(TypedDict):\n",
    "    query: str\n",
    "    context: str\n",
    "    response: str\n",
    "\n",
    "# Define a simple RAG agent workflow\n",
    "def generate(state: GraphState) -> Dict[str, Any]:\n",
    "    query = state['query']\n",
    "    context = state['context']\n",
    "    # Simulate LLM call\n",
    "    response = f\"Based on the context '{context}', the answer to '{query}' is...\"\n",
    "    return {\"response\": response}\n",
    "\n",
    "# Define a dummy retrieve function\n",
    "def retrieve(state):\n",
    "    return {\"context\": \"Dummy context\"}\n",
    "\n",
    "# Create graph\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "# Connect nodes\n",
    "# Define a conditional check\n",
    "def should_continue(state):\n",
    "    # Example: Stop after one iteration\n",
    "    return {\"continue\": False}\n",
    "\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_node(\"continue\", should_continue)\n",
    "workflow.add_edge(\"generate\", \"continue\")\n",
    "\n",
    "# Add conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    \"continue\",\n",
    "    lambda x: \"generate\" if x[\"continue\"] else \"end\",\n",
    "    {\"generate\": \"generate\", \"end\": END}\n",
    ")\n",
    "\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "\n",
    "# Compile and run\n",
    "app = workflow.compile()\n",
    "result = app.invoke({\"query\": \"How do vector databases work?\"})\n",
    "print(result['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ca11bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: [Errno 2] No such file or directory: 'ffmpeg'\n",
      "Please ensure that ffmpeg is installed and accessible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/agent-env/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "# Convert audio to text\n",
    "def transcribe_audio(audio_path):\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(audio_path)\n",
    "    return result[\"text\"]\n",
    "\n",
    "try:\n",
    "    transcription = transcribe_audio(\"audio_sample.mp3\")\n",
    "    print(\"Transcribed text:\", transcription)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Please ensure that ffmpeg is installed and accessible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a7d6f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorflow) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/agent-env/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Epoch 1/3\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.3987 - loss: 1.0988\n",
      "Epoch 2/3\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.3628 - loss: 1.0836\n",
      "Epoch 3/3\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.4527 - loss: 0.9952\n",
      "Model trained successfully\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow\n",
    "import tensorflow as tf # type: ignore\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "import numpy as np\n",
    "\n",
    "# Create a simple LSTM model for text classification\n",
    "vocab_size = 10000\n",
    "embedding_dim = 128\n",
    "max_length = 100\n",
    "num_classes = 3\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    LSTM(64),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Generate dummy data\n",
    "x_train = np.random.randint(0, vocab_size, size=(100, max_length))\n",
    "y_train = np.random.randint(0, num_classes, size=(100,))\n",
    "\n",
    "# Train model\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=16)\n",
    "print(\"Model trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58c5d4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 100/200\n",
      "Vocabulary size: 200/200\n",
      "Generated text: Language\n",
      "\n",
      "Mathematical explanation:\n",
      "1. BPE Tokenization:\n",
      "   - Starting with character vocabulary\n",
      "   - Iteratively finding most frequent adjacent pairs\n",
      "   - Merge operation: (a,b) -> ab\n",
      "   - Final vocabulary size: 200\n",
      "\n",
      "2. N-gram Language Model:\n",
      "   - Context: P(token_n | token_{n-2}, token_{n-1})\n",
      "   - Maximum likelihood estimation:\n",
      "     P(w_i | w_{i-2}, w_{i-1}) = count(w_{i-2}, w_{i-1}, w_i) / count(w_{i-2}, w_{i-1})\n",
      "   - Temperature sampling:\n",
      "     p_t(w_i) ∝ exp(log(p(w_i))/t)\n",
      "     t=1.0: Standard probabilities\n",
      "     t<1.0: More peaked/deterministic\n",
      "     t>1.0: More uniform/random\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class SimpleBPETokenizer:\n",
    "    def __init__(self, vocab_size=1000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}  # token to id\n",
    "        self.inv_vocab = {}  # id to token\n",
    "        self.merges = {}  # merge pairs to token\n",
    "        \n",
    "    def train(self, texts, initial_vocab=None):\n",
    "        \"\"\"Train BPE tokenizer on texts\"\"\"\n",
    "        # Start with character vocabulary\n",
    "        if initial_vocab is None:\n",
    "            # Create character-level vocabulary\n",
    "            chars = Counter()\n",
    "            for text in texts:\n",
    "                chars.update(text)\n",
    "            \n",
    "            # Initialize with characters\n",
    "            self.vocab = {c: i for i, c in enumerate(chars)}\n",
    "            self.inv_vocab = {i: c for i, c in enumerate(chars)}\n",
    "            next_id = len(self.vocab)\n",
    "        else:\n",
    "            self.vocab = initial_vocab\n",
    "            self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "            next_id = max(self.inv_vocab.keys()) + 1\n",
    "        \n",
    "        # Prepare text for merges\n",
    "        split_texts = [[c for c in text] for text in texts]\n",
    "        \n",
    "        # Iteratively merge most common pairs\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            # Count pairs\n",
    "            pairs = Counter()\n",
    "            for text in split_texts:\n",
    "                for i in range(len(text) - 1):\n",
    "                    pair = (text[i], text[i + 1])\n",
    "                    pairs[pair] += 1\n",
    "            \n",
    "            if not pairs:\n",
    "                break\n",
    "                \n",
    "            # Find most frequent pair\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            new_token = best_pair[0] + best_pair[1]\n",
    "            \n",
    "            # Add to vocabulary\n",
    "            self.vocab[new_token] = next_id\n",
    "            self.inv_vocab[next_id] = new_token\n",
    "            self.merges[best_pair] = new_token\n",
    "            next_id += 1\n",
    "            \n",
    "            # Apply merge to all texts\n",
    "            new_split_texts = []\n",
    "            for text in split_texts:\n",
    "                i = 0\n",
    "                new_text = []\n",
    "                while i < len(text):\n",
    "                    if i < len(text) - 1 and (text[i], text[i + 1]) == best_pair:\n",
    "                        new_text.append(new_token)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_text.append(text[i])\n",
    "                        i += 1\n",
    "                new_split_texts.append(new_text)\n",
    "            split_texts = new_split_texts\n",
    "            \n",
    "            if len(self.vocab) % 100 == 0:\n",
    "                print(f\"Vocabulary size: {len(self.vocab)}/{self.vocab_size}\")\n",
    "        \n",
    "        return self.vocab\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize text using learned merges\"\"\"\n",
    "        # Start with characters\n",
    "        tokens = [c for c in text]\n",
    "        \n",
    "        # Apply merges\n",
    "        i = 0\n",
    "        while i < len(tokens) - 1:\n",
    "            pair = (tokens[i], tokens[i + 1])\n",
    "            if pair in self.merges:\n",
    "                tokens[i] = self.merges[pair]\n",
    "                tokens.pop(i + 1)\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        # Convert to token ids\n",
    "        return [self.vocab.get(t, self.vocab.get('<unk>', 0)) for t in tokens]\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Convert token ids back to text\"\"\"\n",
    "        return ''.join(self.inv_vocab.get(tid, '<unk>') for tid in token_ids)\n",
    "\n",
    "\n",
    "# Simple language model for next token prediction\n",
    "class SimpleNGramModel:\n",
    "    def __init__(self, n=3):\n",
    "        self.n = n\n",
    "        self.context_counts = {}  # Store counts of n-grams\n",
    "        self.context_next = {}    # Store next token distributions\n",
    "    \n",
    "    def train(self, token_sequences):\n",
    "        \"\"\"Train on token sequences\"\"\"\n",
    "        for sequence in token_sequences:\n",
    "            # Pad sequence for context\n",
    "            padded = [0] * (self.n - 1) + sequence\n",
    "            \n",
    "            # Collect n-grams and next tokens\n",
    "            for i in range(len(padded) - self.n):\n",
    "                context = tuple(padded[i:i+self.n-1])\n",
    "                next_token = padded[i+self.n-1]\n",
    "                \n",
    "                # Update counts\n",
    "                if context not in self.context_counts:\n",
    "                    self.context_counts[context] = 0\n",
    "                    self.context_next[context] = {}\n",
    "                \n",
    "                self.context_counts[context] += 1\n",
    "                \n",
    "                if next_token not in self.context_next[context]:\n",
    "                    self.context_next[context][next_token] = 0\n",
    "                self.context_next[context][next_token] += 1\n",
    "    \n",
    "    def predict_next_token(self, context, temperature=1.0):\n",
    "        \"\"\"Predict next token given context\"\"\"\n",
    "        # Get last n-1 tokens\n",
    "        context = tuple(context[-(self.n-1):]) if len(context) >= self.n-1 else tuple([0] * (self.n-1 - len(context)) + context)\n",
    "        \n",
    "        # If context not seen, return random from vocabulary\n",
    "        if context not in self.context_next:\n",
    "            return None\n",
    "        \n",
    "        # Get distribution\n",
    "        next_tokens = self.context_next[context]\n",
    "        tokens = list(next_tokens.keys())\n",
    "        counts = np.array(list(next_tokens.values()))\n",
    "        \n",
    "        # Apply temperature\n",
    "        if temperature == 0:  # Greedy\n",
    "            return tokens[np.argmax(counts)]\n",
    "        else:\n",
    "            # Convert counts to probabilities\n",
    "            probs = counts / counts.sum()\n",
    "            \n",
    "            # Apply temperature\n",
    "            logits = np.log(probs)\n",
    "            logits_t = logits / temperature\n",
    "            probs_t = np.exp(logits_t) / np.exp(logits_t).sum()\n",
    "            \n",
    "            # Sample\n",
    "            return np.random.choice(tokens, p=probs_t)\n",
    "\n",
    "# Example usage\n",
    "texts = [\n",
    "    \"Hello world! This is an example of tokenization.\",\n",
    "    \"Byte pair encoding is a data compression technique.\",\n",
    "    \"Language models predict the next token in a sequence.\",\n",
    "    \"Transformers use self-attention mechanisms.\",\n",
    "    \"Neural networks learn from data.\"\n",
    "]\n",
    "\n",
    "# Train tokenizer\n",
    "tokenizer = SimpleBPETokenizer(vocab_size=200)\n",
    "tokenizer.train(texts)\n",
    "\n",
    "# Tokenize texts\n",
    "tokenized_texts = [tokenizer.tokenize(text) for text in texts]\n",
    "\n",
    "# Train language model\n",
    "lm = SimpleNGramModel(n=3)\n",
    "lm.train(tokenized_texts)\n",
    "\n",
    "# Generate some text\n",
    "def generate_text(model, tokenizer, start_text=\"Hello\", length=20, temperature=0.8):\n",
    "    tokens = tokenizer.tokenize(start_text)\n",
    "    \n",
    "    for _ in range(length):\n",
    "        next_token = model.predict_next_token(tokens, temperature=temperature)\n",
    "        if next_token is None:\n",
    "            break\n",
    "        tokens.append(next_token)\n",
    "    \n",
    "    return tokenizer.decode(tokens)\n",
    "\n",
    "# Generate text example\n",
    "generated = generate_text(lm, tokenizer, start_text=\"Language\", length=20)\n",
    "print(\"Generated text:\", generated)\n",
    "\n",
    "# Mathematical explanation\n",
    "print(\"\\nMathematical explanation:\")\n",
    "print(\"1. BPE Tokenization:\")\n",
    "print(\"   - Starting with character vocabulary\")\n",
    "print(\"   - Iteratively finding most frequent adjacent pairs\")\n",
    "print(\"   - Merge operation: (a,b) -> ab\")\n",
    "print(\"   - Final vocabulary size:\", len(tokenizer.vocab))\n",
    "\n",
    "print(\"\\n2. N-gram Language Model:\")\n",
    "print(\"   - Context: P(token_n | token_{n-2}, token_{n-1})\")\n",
    "print(\"   - Maximum likelihood estimation:\")\n",
    "print(\"     P(w_i | w_{i-2}, w_{i-1}) = count(w_{i-2}, w_{i-1}, w_i) / count(w_{i-2}, w_{i-1})\")\n",
    "print(\"   - Temperature sampling:\")\n",
    "print(\"     p_t(w_i) ∝ exp(log(p(w_i))/t)\")\n",
    "print(\"     t=1.0: Standard probabilities\")\n",
    "print(\"     t<1.0: More peaked/deterministic\")\n",
    "print(\"     t>1.0: More uniform/random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed996c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Flat (Exact) index...\n",
      "  Adding vectors...\n",
      "  Built index in 0.01 seconds\n",
      "  Searching...\n",
      "  Average search time: 0.58 ms per query\n",
      "  Results for first query:\n",
      "    1. Vector 76340, Distance: 3342.2617\n",
      "    2. Vector 94418, Distance: 3350.3750\n",
      "    3. Vector 97173, Distance: 3430.5532\n",
      "    4. Vector 78001, Distance: 3444.8921\n",
      "    5. Vector 14813, Distance: 3444.9600\n",
      "\n",
      "  Mathematical details:\n",
      "    - Uses exact L2 distance: sqrt(sum((x_i - y_i)^2))\n",
      "    - Compares each query against all 100,000 vectors\n",
      "\n",
      "Building IVF Flat index...\n",
      "  Training index...\n",
      "  Adding vectors...\n",
      "  Built index in 0.06 seconds\n",
      "  Searching...\n",
      "  Average search time: 0.02 ms per query\n",
      "  Results for first query:\n",
      "    1. Vector 26909, Distance: 3657.9131\n",
      "    2. Vector 93396, Distance: 3788.4683\n",
      "    3. Vector 51201, Distance: 3798.6841\n",
      "    4. Vector 7662, Distance: 3801.4509\n",
      "    5. Vector 36893, Distance: 3802.8486\n",
      "\n",
      "  Mathematical details:\n",
      "    - Partitions vectors into 100 Voronoi cells\n",
      "    - Query identifies nearest centroids first\n",
      "    - Only searches vectors in the nearest cells\n",
      "    - nprobe parameter: 1 (cells searched)\n",
      "\n",
      "Building IVF PQ index...\n",
      "  Training index...\n",
      "  Adding vectors...\n",
      "  Built index in 0.85 seconds\n",
      "  Searching...\n",
      "  Average search time: 0.03 ms per query\n",
      "  Results for first query:\n",
      "    1. Vector 79987, Distance: 3830.5225\n",
      "    2. Vector 6194, Distance: 3833.8411\n",
      "    3. Vector 20446, Distance: 3846.3159\n",
      "    4. Vector 26909, Distance: 3847.6279\n",
      "    5. Vector 45304, Distance: 3852.6826\n",
      "\n",
      "  Mathematical details:\n",
      "    - Combines IVF with Product Quantization\n",
      "    - Splits 128D vectors into 16 subvectors of 8D each\n",
      "    - Each subspace uses 8-bit codes (256 centroids)\n",
      "    - Reduces storage from 512 bytes to ~16+8 bytes per vector\n",
      "\n",
      "Building HNSW index...\n",
      "  Adding vectors...\n",
      "  Built index in 0.97 seconds\n",
      "  Searching...\n",
      "  Average search time: 0.03 ms per query\n",
      "  Results for first query:\n",
      "    1. Vector 78001, Distance: 3444.8921\n",
      "    2. Vector 14813, Distance: 3444.9600\n",
      "    3. Vector 56338, Distance: 3469.8906\n",
      "    4. Vector 96273, Distance: 3476.3406\n",
      "    5. Vector 80148, Distance: 3494.6770\n",
      "\n",
      "  Mathematical details:\n",
      "    - Hierarchical Navigable Small World graph\n",
      "    - Creates multi-layered graph with 32 max connections\n",
      "    - Greedy graph traversal from layer to layer\n",
      "    - efConstruction: 40\n",
      "    - efSearch: 16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import time\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Create synthetic data: 100,000 vectors of 128 dimensions\n",
    "n_vectors = 100000\n",
    "dimension = 128\n",
    "n_clusters = 100\n",
    "data, _ = make_blobs(n_samples=n_vectors, n_features=dimension, centers=n_clusters, random_state=42)\n",
    "\n",
    "# Convert to float32 (required by FAISS)\n",
    "data = data.astype(np.float32)\n",
    "\n",
    "# Create query vectors\n",
    "n_queries = 10\n",
    "queries = np.random.random((n_queries, dimension)).astype(np.float32)\n",
    "\n",
    "# Define indices to compare\n",
    "index_types = {\n",
    "    \"Flat (Exact)\": faiss.IndexFlatL2(dimension),\n",
    "    \"IVF Flat\": faiss.IndexIVFFlat(faiss.IndexFlatL2(dimension), dimension, n_clusters),\n",
    "    \"IVF PQ\": faiss.IndexIVFPQ(faiss.IndexFlatL2(dimension), dimension, n_clusters, 16, 8),\n",
    "    \"HNSW\": faiss.IndexHNSWFlat(dimension, 32)\n",
    "}\n",
    "\n",
    "# Number of nearest neighbors to retrieve\n",
    "k = 5\n",
    "\n",
    "# Train and add vectors to indices\n",
    "for name, index in index_types.items():\n",
    "    print(f\"\\nBuilding {name} index...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train if needed\n",
    "    if isinstance(index, faiss.IndexIVFFlat) or isinstance(index, faiss.IndexIVFPQ):\n",
    "        print(\"  Training index...\")\n",
    "        index.train(data)\n",
    "    \n",
    "    # Add vectors\n",
    "    print(\"  Adding vectors...\")\n",
    "    index.add(data)\n",
    "    \n",
    "    print(f\"  Built index in {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Perform search\n",
    "    print(\"  Searching...\")\n",
    "    search_start = time.time()\n",
    "    distances, indices = index.search(queries, k)\n",
    "    search_time = time.time() - search_start\n",
    "    \n",
    "    # Report results\n",
    "    print(f\"  Average search time: {search_time / n_queries * 1000:.2f} ms per query\")\n",
    "    print(f\"  Results for first query:\")\n",
    "    for i in range(k):\n",
    "        print(f\"    {i+1}. Vector {indices[0][i]}, Distance: {distances[0][i]:.4f}\")\n",
    "    \n",
    "    # Mathematical details explanation\n",
    "    print(\"\\n  Mathematical details:\")\n",
    "    if name == \"Flat (Exact)\":\n",
    "        print(\"    - Uses exact L2 distance: sqrt(sum((x_i - y_i)^2))\")\n",
    "        print(\"    - Compares each query against all 100,000 vectors\")\n",
    "    elif name == \"IVF Flat\":\n",
    "        print(\"    - Partitions vectors into 100 Voronoi cells\")\n",
    "        print(\"    - Query identifies nearest centroids first\")\n",
    "        print(\"    - Only searches vectors in the nearest cells\")\n",
    "        print(f\"    - nprobe parameter: {index.nprobe} (cells searched)\")\n",
    "    elif name == \"IVF PQ\":\n",
    "        print(\"    - Combines IVF with Product Quantization\")\n",
    "        print(\"    - Splits 128D vectors into 16 subvectors of 8D each\")\n",
    "        print(\"    - Each subspace uses 8-bit codes (256 centroids)\")\n",
    "        print(\"    - Reduces storage from 512 bytes to ~16+8 bytes per vector\")\n",
    "    elif name == \"HNSW\":\n",
    "        print(\"    - Hierarchical Navigable Small World graph\")\n",
    "        print(\"    - Creates multi-layered graph with 32 max connections\")\n",
    "        print(\"    - Greedy graph traversal from layer to layer\")\n",
    "        print(\"    - efConstruction:\", index.hnsw.efConstruction)\n",
    "        print(\"    - efSearch:\", index.hnsw.efSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c857f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "Attention weights shape: torch.Size([2, 8, 10, 10])\n",
      "\n",
      "Attention weights (first head, first sequence):\n",
      "[[0.08636466 0.08893646 0.09673692 0.11184515 0.09727592 0.14506705\n",
      "  0.0883042  0.08184995 0.10140611 0.10221367]\n",
      " [0.16273554 0.12478302 0.07462385 0.11172861 0.06013608 0.07909601\n",
      "  0.06422267 0.11619847 0.12035584 0.08611996]\n",
      " [0.12352862 0.10443546 0.07732977 0.03498055 0.06023482 0.0951709\n",
      "  0.13528565 0.15707369 0.06947392 0.14248656]\n",
      " [0.07454232 0.15310286 0.09102074 0.09851746 0.09981946 0.14370447\n",
      "  0.07783068 0.09990502 0.09184451 0.06971247]\n",
      " [0.09165742 0.10104419 0.11459634 0.0679939  0.094329   0.13018431\n",
      "  0.08362576 0.05784969 0.13539869 0.12332077]\n",
      " [0.14267313 0.08702393 0.10666597 0.06055981 0.13048515 0.09498405\n",
      "  0.1230271  0.05514885 0.09216403 0.10726799]\n",
      " [0.03731251 0.07823309 0.12634353 0.06967332 0.11493493 0.15215053\n",
      "  0.100695   0.13017876 0.04352712 0.1469512 ]\n",
      " [0.08298459 0.12039216 0.08526737 0.05858592 0.08295417 0.09402767\n",
      "  0.067625   0.20457226 0.05613871 0.14745213]\n",
      " [0.1071443  0.12840073 0.16789463 0.03708313 0.11437774 0.11628935\n",
      "  0.11510593 0.08038183 0.06521969 0.06810262]\n",
      " [0.06726222 0.12998772 0.14453359 0.13221614 0.10236911 0.09449179\n",
      "  0.0836902  0.06838191 0.1193252  0.05774221]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.query = nn.Linear(embed_size, embed_size)\n",
    "        self.key = nn.Linear(embed_size, embed_size)\n",
    "        self.value = nn.Linear(embed_size, embed_size)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # Get batch size\n",
    "        N = query.shape[0]\n",
    "        \n",
    "        # Get sequence length\n",
    "        query_len, key_len, value_len = query.shape[1], key.shape[1], value.shape[1]\n",
    "        \n",
    "        # 1. Linear projections\n",
    "        Q = self.query(query)  # (N, query_len, embed_size)\n",
    "        K = self.key(key)      # (N, key_len, embed_size)\n",
    "        V = self.value(value)  # (N, value_len, embed_size)\n",
    "        \n",
    "        # 2. Split into multiple heads\n",
    "        Q = Q.reshape(N, query_len, self.heads, self.head_dim)\n",
    "        K = K.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        V = V.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        \n",
    "        # 3. Transpose for matrix multiplication\n",
    "        Q = Q.transpose(1, 2)  # (N, heads, query_len, head_dim)\n",
    "        K = K.transpose(1, 2)  # (N, heads, key_len, head_dim)\n",
    "        V = V.transpose(1, 2)  # (N, heads, value_len, head_dim)\n",
    "        \n",
    "        # 4. Calculate attention scores\n",
    "        # Attention(Q,K,V) = softmax(QK^T/√d_k)V\n",
    "        \n",
    "        # Matrix multiplication: QK^T\n",
    "        energy = torch.matmul(Q, K.transpose(-2, -1))  # (N, heads, query_len, key_len)\n",
    "        \n",
    "        # Scale: QK^T/√d_k\n",
    "        scaling_factor = self.head_dim ** 0.5\n",
    "        scaled_energy = energy / scaling_factor\n",
    "        \n",
    "        # Apply mask (optional)\n",
    "        if mask is not None:\n",
    "            scaled_energy = scaled_energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        \n",
    "        # 5. Apply softmax to get attention weights\n",
    "        attention = F.softmax(scaled_energy, dim=-1)  # (N, heads, query_len, key_len)\n",
    "        \n",
    "        # 6. Multiply by values\n",
    "        out = torch.matmul(attention, V)  # (N, heads, query_len, head_dim)\n",
    "        \n",
    "        # 7. Reshape and concatenate heads\n",
    "        out = out.transpose(1, 2)  # (N, query_len, heads, head_dim)\n",
    "        out = out.reshape(N, query_len, self.embed_size)  # (N, query_len, embed_size)\n",
    "        \n",
    "        # 8. Final linear layer\n",
    "        out = self.fc_out(out)  # (N, query_len, embed_size)\n",
    "        \n",
    "        return out, attention\n",
    "\n",
    "# Example usage\n",
    "def test_attention():\n",
    "    # Parameters\n",
    "    batch_size = 2\n",
    "    sequence_length = 10\n",
    "    embedding_dim = 512\n",
    "    num_heads = 8\n",
    "    \n",
    "    # Create random input tensors\n",
    "    x = torch.randn(batch_size, sequence_length, embedding_dim)\n",
    "    \n",
    "    # Initialize self-attention layer\n",
    "    self_attention = SelfAttention(embedding_dim, num_heads)\n",
    "    \n",
    "    # Forward pass\n",
    "    output, attention_weights = self_attention(x, x, x)\n",
    "    \n",
    "    # Output shapes\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "    \n",
    "    # Visualize one attention head for first sequence\n",
    "    print(\"\\nAttention weights (first head, first sequence):\")\n",
    "    print(attention_weights[0, 0].detach().numpy())\n",
    "\n",
    "test_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15bd26b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How is AI used in medicine?\n",
      "\n",
      "Ranked Results:\n",
      "Document 4: Healthcare data privacy is essential when implementing AI systems.\n",
      "Similarity Score: 0.4998\n",
      "--------------------------------------------------\n",
      "Document 0: Artificial intelligence is revolutionizing healthcare with predictive analytics.\n",
      "Similarity Score: 0.4890\n",
      "--------------------------------------------------\n",
      "Document 2: Natural language processing helps in analyzing clinical notes.\n",
      "Similarity Score: 0.3850\n",
      "--------------------------------------------------\n",
      "Document 3: Deep learning systems are being used for drug discovery.\n",
      "Similarity Score: 0.3765\n",
      "--------------------------------------------------\n",
      "Document 1: Machine learning models can detect patterns in medical images.\n",
      "Similarity Score: 0.3513\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Initialize embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # 384-dimension embeddings\n",
    "\n",
    "# Example document corpus\n",
    "documents = [\n",
    "    \"Artificial intelligence is revolutionizing healthcare with predictive analytics.\",\n",
    "    \"Machine learning models can detect patterns in medical images.\",\n",
    "    \"Natural language processing helps in analyzing clinical notes.\",\n",
    "    \"Deep learning systems are being used for drug discovery.\",\n",
    "    \"Healthcare data privacy is essential when implementing AI systems.\"\n",
    "]\n",
    "\n",
    "# Create document embeddings\n",
    "document_embeddings = model.encode(documents)\n",
    "\n",
    "# User query\n",
    "query = \"How is AI used in medicine?\"\n",
    "query_embedding = model.encode([query])[0]\n",
    "\n",
    "# Calculate cosine similarity between query and all documents\n",
    "similarities = []\n",
    "for i, doc_embedding in enumerate(document_embeddings):\n",
    "    # Cosine similarity formula: cos(θ) = (A·B)/(||A||·||B||)\n",
    "    dot_product = np.dot(query_embedding, doc_embedding)\n",
    "    query_norm = np.linalg.norm(query_embedding)\n",
    "    doc_norm = np.linalg.norm(doc_embedding)\n",
    "    cosine_sim = dot_product / (query_norm * doc_norm)\n",
    "    similarities.append((i, cosine_sim))\n",
    "\n",
    "# Sort by similarity (highest first)\n",
    "ranked_results = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display ranked results\n",
    "print(\"Query:\", query)\n",
    "print(\"\\nRanked Results:\")\n",
    "for doc_id, similarity in ranked_results:\n",
    "    print(f\"Document {doc_id}: {documents[doc_id]}\")\n",
    "    print(f\"Similarity Score: {similarity:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3ec23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x107e93800>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/agent-env/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import dash\n",
    "from dash import html, dcc, Input, Output, State\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "\n",
    "# Document class\n",
    "class Document:\n",
    "    def __init__(self, doc_id: str, text: str, metadata: Optional[Dict[str, Any]] = None):\n",
    "        self.id = doc_id\n",
    "        self.text = text\n",
    "        self.metadata = metadata if metadata else {}\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Document(id={self.id}, text={self.text[:50]}...)\"\n",
    "\n",
    "# Chunker class\n",
    "class Chunker:\n",
    "    def __init__(self, chunk_size: int = 512, chunk_overlap: int = 128):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def chunk_document(self, doc: Document) -> List[Document]:\n",
    "        text = doc.text\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = min(start + self.chunk_size, len(text))\n",
    "            sentence_end = max(\n",
    "                text.rfind('. ', start, end),\n",
    "                text.rfind('! ', start, end),\n",
    "                text.rfind('? ', start, end)\n",
    "            )\n",
    "            if sentence_end > start + 100:\n",
    "                end = sentence_end + 1\n",
    "            chunk_text = text[start:end].strip()\n",
    "            chunk_id = f\"{doc.id}_chunk_{len(chunks)}\"\n",
    "            chunk_metadata = doc.metadata.copy()\n",
    "            chunk_metadata.update({\n",
    "                'parent_id': doc.id,\n",
    "                'chunk_id': len(chunks),\n",
    "                'start_char': start,\n",
    "                'end_char': end\n",
    "            })\n",
    "            chunks.append(Document(chunk_id, chunk_text, chunk_metadata))\n",
    "            start = end - self.chunk_overlap\n",
    "            if start >= len(text) - 50:\n",
    "                break\n",
    "        return chunks\n",
    "\n",
    "# SemanticEmbedder class\n",
    "class SemanticEmbedder:\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    def embed(self, texts: List[str]) -> np.ndarray:\n",
    "        encoded_input = self.tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "        embeddings = self._mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        return embeddings.numpy()\n",
    "\n",
    "# HybridRetriever class\n",
    "class HybridRetriever:\n",
    "    def __init__(self, documents: List[Document], semantic_weight: float = 0.7):\n",
    "        self.documents = documents\n",
    "        self.doc_texts = [doc.text for doc in documents]\n",
    "        self.semantic_weight = semantic_weight\n",
    "        tokenized_docs = [text.lower().split() for text in self.doc_texts]\n",
    "        self.bm25 = BM25Okapi(tokenized_docs)\n",
    "        self.embedder = SemanticEmbedder()\n",
    "        self.doc_embeddings = self.embedder.embed(self.doc_texts)\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5) -> List[Document]:\n",
    "        tokenized_query = query.lower().split()\n",
    "        bm25_scores = np.array(self.bm25.get_scores(tokenized_query))\n",
    "        if np.max(bm25_scores) > 0:\n",
    "            bm25_scores = bm25_scores / np.max(bm25_scores)\n",
    "        query_embedding = self.embedder.embed([query])[0]\n",
    "        semantic_scores = np.dot(self.doc_embeddings, query_embedding)\n",
    "        combined_scores = self.semantic_weight * semantic_scores + (1 - self.semantic_weight) * bm25_scores\n",
    "        top_indices = np.argsort(-combined_scores)[:top_k]\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            doc = self.documents[idx]\n",
    "            doc.metadata['retrieval_scores'] = {\n",
    "                'combined_score': float(combined_scores[idx]),\n",
    "                'semantic_score': float(semantic_scores[idx]),\n",
    "                'bm25_score': float(bm25_scores[idx])\n",
    "            }\n",
    "            results.append(doc)\n",
    "        return results\n",
    "\n",
    "# Reranker class\n",
    "class Reranker:\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "\n",
    "    def fit(self, documents: List[Document]):\n",
    "        self.tfidf.fit([doc.text for doc in documents])\n",
    "        return self\n",
    "\n",
    "    def rerank(self, query: str, docs: List[Document], top_k: int = None) -> List[Document]:\n",
    "        if not docs:\n",
    "            return []\n",
    "        doc_texts = [doc.text for doc in docs]\n",
    "        doc_vectors = self.tfidf.transform(doc_texts)\n",
    "        query_vector = self.tfidf.transform([query])\n",
    "        similarities = np.dot(doc_vectors, query_vector.T).toarray().flatten()\n",
    "        scored_docs = list(zip(docs, similarities))\n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        if top_k:\n",
    "            scored_docs = scored_docs[:top_k]\n",
    "        result_docs = []\n",
    "        for doc, score in scored_docs:\n",
    "            doc.metadata['reranking_score'] = float(score)\n",
    "            result_docs.append(doc)\n",
    "        return result_docs\n",
    "\n",
    "# RAGSystem class\n",
    "class RAGSystem:\n",
    "    def __init__(self, documents: List[Document]):\n",
    "        self.chunker = Chunker(chunk_size=512, chunk_overlap=128)\n",
    "        self.chunks = []\n",
    "        for doc in documents:\n",
    "            self.chunks.extend(self.chunker.chunk_document(doc))\n",
    "        self.retriever = HybridRetriever(self.chunks)\n",
    "        self.reranker = Reranker().fit(self.chunks)\n",
    "\n",
    "    def answer_query(self, query: str, num_chunks: int = 5) -> Dict[str, Any]:\n",
    "        retrieved_chunks = self.retriever.retrieve(query, top_k=num_chunks * 2)\n",
    "        reranked_chunks = self.reranker.rerank(query, retrieved_chunks, top_k=num_chunks)\n",
    "        context = \"\\n\\n\".join([f\"[Document {i+1}]: {chunk.text}\" for i, chunk in enumerate(reranked_chunks)])\n",
    "        answer = self._mock_generate_answer(query, context)\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"answer\": answer,\n",
    "            \"context\": context,\n",
    "            \"sources\": [chunk.metadata for chunk in reranked_chunks]\n",
    "        }\n",
    "\n",
    "    def _mock_generate_answer(self, query: str, context: str) -> str:\n",
    "        return f\"This is a mock answer to your query: '{query}'\\n\\nContext:\\n{context[:500]}...\"\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    Document(\"doc1\", \"Artificial intelligence is a branch of computer science that focuses on building smart machines.\"),\n",
    "    Document(\"doc2\", \"Machine learning is a subset of AI that gives computers the ability to learn from data.\"),\n",
    "    Document(\"doc3\", \"Natural language processing is a field of AI that enables machines to understand human language.\"),\n",
    "]\n",
    "\n",
    "# Initialize RAG system\n",
    "rag_system = RAGSystem(docs)\n",
    "\n",
    "# Dash app setup\n",
    "app = dash.Dash(__name__)\n",
    "app.title = \"RAG Demo\"\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H2(\"Retrieval-Augmented Generation (RAG) System\"),\n",
    "    dcc.Textarea(id=\"query-input\", placeholder=\"Enter your query here...\", style={\"width\": \"100%\", \"height\": 100}),\n",
    "    html.Button(\"Submit\", id=\"submit-btn\", n_clicks=0),\n",
    "    html.Hr(),\n",
    "    html.Div(id=\"answer-output\", style={\"whiteSpace\": \"pre-wrap\", \"marginTop\": 20}),\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"answer-output\", \"children\"),\n",
    "    Input(\"submit-btn\", \"n_clicks\"),\n",
    "    State(\"query-input\", \"value\")\n",
    ")\n",
    "def handle_query(n_clicks, query):\n",
    "    if n_clicks > 0 and query:\n",
    "        result = rag_system.answer_query(query)\n",
    "        return f\"Answer:\\n{result['answer']}\\n\\nSources:\\n\" + \"\\n\".join(\n",
    "            f\"- {src['parent_id']} (score: {src.get('reranking_score', 0):.4f})\"\n",
    "            for src in result['sources']\n",
    "        )\n",
    "    return \"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85618300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class Document:\n",
    "    \"\"\"Represents a document with metadata.\"\"\"\n",
    "    def __init__(self, doc_id: str, text: str, metadata: Optional[Dict[str, Any]] = None):\n",
    "        self.id = doc_id\n",
    "        self.text = text\n",
    "        self.metadata = metadata if metadata else {}\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"Document(id={self.id}, text={self.text[:50]}...)\"\n",
    "\n",
    "class Chunker:\n",
    "    \"\"\"Chunks documents into smaller pieces.\"\"\"\n",
    "    def __init__(self, chunk_size: int = 512, chunk_overlap: int = 128):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "    def chunk_document(self, doc: Document) -> List[Document]:\n",
    "        \"\"\"Split document into chunks with overlap.\"\"\"\n",
    "        text = doc.text\n",
    "        chunks = []\n",
    "        \n",
    "        # Simple character-based chunking\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = min(start + self.chunk_size, len(text))\n",
    "            \n",
    "            # Try to end at a sentence boundary\n",
    "            if end < len(text):\n",
    "                # Look for sentence boundary (., !, ?)\n",
    "                sentence_end = max(\n",
    "                    text.rfind('. ', start, end),\n",
    "                    text.rfind('! ', start, end),\n",
    "                    text.rfind('? ', start, end)\n",
    "                )\n",
    "                \n",
    "                if sentence_end > start + 100:  # Only use if not too short\n",
    "                    end = sentence_end + 1\n",
    "            \n",
    "            # Create chunk document\n",
    "            chunk_text = text[start:end].strip()\n",
    "            chunk_id = f\"{doc.id}_chunk_{len(chunks)}\"\n",
    "            chunk_metadata = doc.metadata.copy()\n",
    "            chunk_metadata['parent_id'] = doc.id\n",
    "            chunk_metadata['chunk_id'] = len(chunks)\n",
    "            chunk_metadata['start_char'] = start\n",
    "            chunk_metadata['end_char'] = end\n",
    "            \n",
    "            chunks.append(Document(chunk_id, chunk_text, chunk_metadata))\n",
    "            \n",
    "            # Move start position for next chunk (with overlap)\n",
    "            start = end - self.chunk_overlap\n",
    "            if start >= len(text) - 50:  # Avoid tiny chunks at the end\n",
    "                break\n",
    "                \n",
    "        return chunks\n",
    "\n",
    "class SemanticEmbedder:\n",
    "    \"\"\"Creates embeddings using a transformer model.\"\"\"\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"Mean pooling to get sentence embeddings.\"\"\"\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    def embed(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Create embeddings for a list of texts.\"\"\"\n",
    "        # Tokenize\n",
    "        encoded_input = self.tokenizer(\n",
    "            texts, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "        \n",
    "        # Mean pooling\n",
    "        embeddings = self._mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        \n",
    "        # Normalize\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        return embeddings.numpy()\n",
    "\n",
    "class HybridRetriever:\n",
    "    \"\"\"Retrieves documents using both lexical and semantic search.\"\"\"\n",
    "    def __init__(self, documents: List[Document], semantic_weight: float = 0.7, \n",
    "                 precomputed_embeddings: Optional[np.ndarray] = None):\n",
    "        self.documents = documents\n",
    "        self.doc_texts = [doc.text for doc in documents]\n",
    "        self.semantic_weight = semantic_weight\n",
    "        \n",
    "        # Lexical search with BM25\n",
    "        tokenized_docs = [text.lower().split() for text in self.doc_texts]\n",
    "        self.bm25 = BM25Okapi(tokenized_docs)\n",
    "        \n",
    "\n",
    "        # Semantic search - use precomputed embeddings if provided\n",
    "        self.embedder = SemanticEmbedder()\n",
    "        if precomputed_embeddings is not None:\n",
    "            self.doc_embeddings = precomputed_embeddings\n",
    "        else:\n",
    "            self.doc_embeddings = self.embedder.embed(self.doc_texts)\n",
    "class Reranker:\n",
    "    \"\"\"Reranks retrieved documents based on query relevance.\"\"\"\n",
    "    def __init__(self, cross_encoder_name: str = None):\n",
    "        # In a real implementation, would use a cross-encoder model\n",
    "        # Here we'll implement a simple reranker based on term overlap\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        \n",
    "    def fit(self, documents: List[Document]):\n",
    "        \"\"\"Fit the reranker on documents.\"\"\"\n",
    "        self.tfidf.fit([doc.text for doc in documents])\n",
    "        return self\n",
    "        \n",
    "    def rerank(self, query: str, docs: List[Document], top_k: int = None) -> List[Document]:\n",
    "        \"\"\"Rerank documents based on query relevance.\"\"\"\n",
    "        if not docs:\n",
    "            return []\n",
    "            \n",
    "        # Create TF-IDF vectors\n",
    "        doc_texts = [doc.text for doc in docs]\n",
    "        doc_vectors = self.tfidf.transform(doc_texts)\n",
    "        query_vector = self.tfidf.transform([query])\n",
    "        \n",
    "        # Calculate similarity scores\n",
    "        similarities = np.dot(doc_vectors, query_vector.T).toarray().flatten()\n",
    "        \n",
    "        # Sort documents by similarity\n",
    "        scored_docs = list(zip(docs, similarities))\n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Take top_k if specified\n",
    "        if top_k:\n",
    "            scored_docs = scored_docs[:top_k]\n",
    "        \n",
    "        # Update metadata with reranking score\n",
    "        result_docs = []\n",
    "        for doc, score in scored_docs:\n",
    "            doc.metadata['reranking_score'] = float(score)\n",
    "            result_docs.append(doc)\n",
    "            \n",
    "        return result_docs\n",
    "\n",
    "class RAGSystem:\n",
    "    \"\"\"Complete RAG system for retrieval and generation.\"\"\"\n",
    "    def __init__(self, documents: List[Document]):\n",
    "        # Process documents\n",
    "        self.chunker = Chunker(chunk_size=512, chunk_overlap=128)\n",
    "        self.chunks = []\n",
    "        for doc in documents:\n",
    "            self.chunks.extend(self.chunker.chunk_document(doc))\n",
    "        \n",
    "        print(f\"Created {len(self.chunks)} chunks from {len(documents)} documents\")\n",
    "        \n",
    "        # Initialize retriever and reranker\n",
    "        self.retriever = HybridRetriever(self.chunks, semantic_weight=0.7)\n",
    "        self.reranker = Reranker().fit(self.chunks)\n",
    "        \n",
    "        # For a real system, would initialize an LLM here\n",
    "        \n",
    "    def answer_query(self, query: str, num_chunks: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Answer a query using RAG.\"\"\"\n",
    "        # 1. Retrieve relevant chunks\n",
    "        retrieved_chunks = self.retriever.retrieve(query, top_k=num_chunks*2)\n",
    "        \n",
    "        # 2. Rerank chunks\n",
    "        reranked_chunks = self.reranker.rerank(query, retrieved_chunks, top_k=num_chunks)\n",
    "        \n",
    "        # 3. Create context\n",
    "        context = \"\\n\\n\".join([f\"[Document {i+1}]: {chunk.text}\" \n",
    "                              for i, chunk in enumerate(reranked_chunks)])\n",
    "        \n",
    "        # 4. Generate answer (in a real system, this would callclass RAGSystem:\n",
    "    \"\"\"Complete RAG system for retrieval and generation.\"\"\"\n",
    "    def __init__(self, documents: List[Document]):\n",
    "        # Process documents\n",
    "        self.chunker = Chunker(chunk_size=512, chunk_overlap=128)\n",
    "        self.chunks = []\n",
    "        for doc in documents:\n",
    "            self.chunks.extend(self.chunker.chunk_document(doc))\n",
    "        \n",
    "        print(f\"Created {len(self.chunks)} chunks from {len(documents)} documents\")\n",
    "        \n",
    "        # Initialize retriever and reranker\n",
    "        self.retriever = HybridRetriever(self.chunks, semantic_weight=0.7)\n",
    "        self.reranker = Reranker().fit(self.chunks)\n",
    "        \n",
    "        # For a real system, would initialize an LLM here\n",
    "        \n",
    "    def answer_query(self, query: str, num_chunks: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Answer a query using RAG.\"\"\"\n",
    "        # 1. Retrieve relevant chunks\n",
    "        retrieved_chunks = self.retriever.retrieve(query, top_k=num_chunks*2)\n",
    "        \n",
    "        # 2. Rerank chunks\n",
    "        reranked_chunks = self.reranker.rerank(query, retrieved_chunks, top_k=num_chunks)\n",
    "        \n",
    "        # 3. Create context\n",
    "        context = \"\\n\\n\".join([f\"[Document {i+1}]: {chunk.text}\" \n",
    "                              for i, chunk in enumerate(reranked_chunks)])\n",
    "        \n",
    "        # 4. Generate answer (in a real system, this would call an LLM API)\n",
    "        # Here we'll simulate a simple response generation\n",
    "        answer = self._generate_answer(query, reranked_chunks)\n",
    "        \n",
    "        # 5. Return complete response with metadata\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"answer\": answer,\n",
    "            \"context\": context,\n",
    "            \"source_chunks\": reranked_chunks,\n",
    "        }\n",
    "    \n",
    "    def _generate_answer(self, query: str, chunks: List[Document]) -> str:\n",
    "        \"\"\"Simulate generating an answer from chunks.\"\"\"\n",
    "        # In a real implementation, this would prompt an LLM with the query and chunks\n",
    "        # For this example, we'll create a simple extractive answer\n",
    "        \n",
    "        if not chunks:\n",
    "            return \"I couldn't find relevant information to answer your question.\"\n",
    "        \n",
    "        # Simple extractive approach: find sentences most similar to query\n",
    "        all_text = \" \".join([chunk.text for chunk in chunks])\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', all_text)\n",
    "        \n",
    "        # Calculate TF-IDF similarity between query and each sentence\n",
    "        tfidf = TfidfVectorizer().fit([query] + sentences)\n",
    "        query_vec = tfidf.transform([query])\n",
    "        sentence_vecs = tfidf.transform(sentences)\n",
    "        \n",
    "        # Get similarity scores\n",
    "        similarities = np.dot(sentence_vecs, query_vec.T).toarray().flatten()\n",
    "        \n",
    "        # Get top 3 most relevant sentences\n",
    "        top_indices = np.argsort(-similarities)[:3]\n",
    "        top_sentences = [sentences[idx] for idx in top_indices if similarities[idx] > 0]\n",
    "        \n",
    "        if not top_sentences:\n",
    "            return \"I found some information but couldn't generate a specific answer to your question.\"\n",
    "        \n",
    "        # Combine sentences into an answer\n",
    "        answer = \" \".join(top_sentences)\n",
    "        \n",
    "        # Add citation\n",
    "        sources = \", \".join([f\"Document {chunk.metadata['parent_id']}\" for chunk in chunks[:3]])\n",
    "        answer += f\"\\n\\nSources: {sources}\"\n",
    "        \n",
    "        return answer\n",
    "\n",
    "\n",
    "class RAGEvaluator:\n",
    "    \"\"\"Evaluates RAG system performance.\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: RAGSystem):\n",
    "        self.rag_system = rag_system\n",
    "        \n",
    "    def evaluate_retrieval(self, queries: List[str], relevant_doc_ids: List[List[str]]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate retrieval performance using precision, recall, and F1.\"\"\"\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        f1_scores = []\n",
    "        \n",
    "        for query, rel_ids in zip(queries, relevant_doc_ids):\n",
    "            # Get system results\n",
    "            result = self.rag_system.answer_query(query)\n",
    "            retrieved_chunks = result[\"source_chunks\"]\n",
    "            retrieved_parent_ids = set([chunk.metadata['parent_id'] for chunk in retrieved_chunks])\n",
    "            \n",
    "            rel_ids_set = set(rel_ids)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            if retrieved_parent_ids:\n",
    "                precision = len(rel_ids_set.intersection(retrieved_parent_ids)) / len(retrieved_parent_ids)\n",
    "            else:\n",
    "                precision = 0.0\n",
    "                \n",
    "            if rel_ids:\n",
    "                recall = len(rel_ids_set.intersection(retrieved_parent_ids)) / len(rel_ids_set)\n",
    "            else:\n",
    "                recall = 1.0 if not retrieved_parent_ids else 0.0\n",
    "                \n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "            \n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "            \n",
    "        # Calculate averages\n",
    "        avg_precision = np.mean(precision_scores)\n",
    "        avg_recall = np.mean(recall_scores)\n",
    "        avg_f1 = np.mean(f1_scores)\n",
    "        \n",
    "        return {\n",
    "            \"precision\": avg_precision,\n",
    "            \"recall\": avg_recall,\n",
    "            \"f1\": avg_f1\n",
    "        }\n",
    "    \n",
    "    def evaluate_answer_relevance(self, queries: List[str], expert_answers: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate answer relevance and correctness.\n",
    "        \n",
    "        In a real implementation, this would use methods like:\n",
    "        1. BLEU or ROUGE scores\n",
    "        2. BERTScore or other embedding similarity\n",
    "        3. LLM-based evaluation\n",
    "        \n",
    "        Here we'll implement a simple TF-IDF cosine similarity.\n",
    "        \"\"\"\n",
    "        similarities = []\n",
    "        \n",
    "        for query, expert_answer in zip(queries, expert_answers):\n",
    "            # Get system answer\n",
    "            result = self.rag_system.answer_query(query)\n",
    "            system_answer = result[\"answer\"]\n",
    "            \n",
    "            # Calculate similarity using TF-IDF\n",
    "            tfidf = TfidfVectorizer().fit([expert_answer, system_answer])\n",
    "            expert_vec = tfidf.transform([expert_answer])\n",
    "            system_vec = tfidf.transform([system_answer])\n",
    "            \n",
    "            similarity = cosine_similarity(expert_vec, system_vec)[0][0]\n",
    "            similarities.append(similarity)\n",
    "        \n",
    "        return {\n",
    "            \"answer_similarity\": np.mean(similarities)\n",
    "        }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def demo_rag_system():\n",
    "    # Create sample documents\n",
    "    documents = [\n",
    "        Document(\"doc1\", \"Neural networks are computing systems vaguely inspired by the biological neural networks that constitute animal brains. An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron receives signals then processes them and can signal neurons connected to it.\"),\n",
    "        Document(\"doc2\", \"Generative AI refers to artificial intelligence systems capable of generating new content, such as text, images, audio, and synthetic data. These systems learn patterns from existing data and use this knowledge to create new, similar but unique outputs. Popular examples include GPT (text), DALL-E (images), and WaveNet (audio).\"),\n",
    "        #Document(\"doc3\", \"Retrieval-Augmented Generation (RAG) combines retrieval-based and generation-based approaches for natural language processing tasks. Instead of relying solely on parametric knowledge stored in the model weights, RAG retrieves relevant documents or passages from an external knowledge source to provide context for generation. This approach helps ground the model's outputs in factual information and reduces hallucinations.\"),\n",
    "        #Document(\"doc4\", \"Vector databases are specialized database systems designed to store, index, and query high-dimensional vectors efficiently. These vectors typically represent embeddings of text, images, or other data types in a semantic space. Popular vector databases include FAISS (Facebook AI Similarity Search), Pinecone, Weaviate, and Milvus. They support operations like nearest neighbor search using various distance metrics.\"),\n",
    "        #Document(\"doc5\", \"Attention mechanisms in neural networks allow models to focus on specific parts of the input when producing output. The transformer architecture, introduced in the paper 'Attention is All You Need,' uses self-attention to weigh the importance of different parts of the input data. This has been revolutionary for natural language processing and other sequence modeling tasks.\")\n",
    "    ]\n",
    "    \n",
    "    # Initialize RAG system\n",
    "    rag = RAGSystem(documents)\n",
    "    \n",
    "    # Example query\n",
    "    query = \"How do vector databases work with RAG systems?\"\n",
    "    result = rag.answer_query(query)\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"\\nAnswer: {result['answer']}\")\n",
    "    print(f\"\\nRetrieved chunks:\")\n",
    "    for i, chunk in enumerate(result['source_chunks']):\n",
    "        scores = chunk.metadata.get('retrieval_scores', {})\n",
    "        print(f\"\\n[{i+1}] {chunk.id} (Score: {scores.get('combined_score', 0):.4f})\")\n",
    "        print(f\"    - Semantic: {scores.get('semantic_score', 0):.4f}, BM25: {scores.get('bm25_score', 0):.4f}\")\n",
    "        print(f\"    - Text: {chunk.text[:100]}...\")\n",
    "    \n",
    "    # Evaluate system (with mock data)\n",
    "    evaluator = RAGEvaluator(rag)\n",
    "    eval_queries = [\n",
    "        \"What are neural networks?\",\n",
    "        \"How does generative AI work?\",\n",
    "        \"Explain RAG systems\"\n",
    "    ]\n",
    "    relevant_docs = [\n",
    "        [\"doc1\", \"doc2\"]\n",
    "        #[\"doc2\"],\n",
    "        #[\"doc3\", \"doc4\"]\n",
    "    ]\n",
    "    expert_answers = [\n",
    "        \"Neural networks are computing systems inspired by biological neural networks in animal brains.\",\n",
    "        \"Generative AI systems learn patterns from existing data to create new, similar but unique outputs like text, images and audio.\",\n",
    "        \"Retrieval-Augmented Generation (RAG) combines retrieval and generation approaches by accessing external knowledge sources to provide context for generation.\"\n",
    "    ]\n",
    "    \n",
    "    retrieval_metrics = evaluator.evaluate_retrieval(eval_queries, relevant_docs)\n",
    "    relevance_metrics = evaluator.evaluate_answer_relevance(eval_queries, expert_answers)\n",
    "    \n",
    "    print(\"\\n=== Evaluation Results ===\")\n",
    "    print(f\"Retrieval Metrics: Precision={retrieval_metrics['precision']:.2f}, \" +\n",
    "          f\"Recall={retrieval_metrics['recall']:.2f}, F1={retrieval_metrics['f1']:.2f}\")\n",
    "    print(f\"Answer Relevance: {relevance_metrics['answer_similarity']:.2f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo_rag_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2446d1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/saviz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏱️ Starting fit process...\n",
      "📄 Splitting documents...\n",
      "📚 Building TF-IDF index...\n",
      "🧠 Generating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60894c902d24ca1b57fd2b40ec69042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fit completed in 0.01s\n",
      "\n",
      "\n",
      "🧠 Query: \"What is the capital of India?\"\n",
      "✅ Completed in 0.00s\n",
      "\n",
      "📢 Answer: \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "\n",
    "# NLTK Setup\n",
    "nltk.download('punkt', download_dir='/Users/saviz/nltk_data')\n",
    "nltk.data.path.append('/Users/saviz/nltk_data')\n",
    "\n",
    "class SimpleRAG:\n",
    "    def __init__(self, top_k=5, chunk_size=100, overlap=20):\n",
    "        self.top_k = top_k\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        self.external_knowledge = {\n",
    "            \"capital of iran\": \"Tehran\",\n",
    "            \"tehran\": \"Capital of Iran\",\n",
    "        }\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        return re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text.lower())\n",
    "\n",
    "    def _split_text(self, text):\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            words = nltk.word_tokenize(sentence)\n",
    "            if current_length + len(words) <= self.chunk_size:\n",
    "                current_chunk.extend(words)\n",
    "                current_length += len(words)\n",
    "            else:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = words[-self.overlap:]\n",
    "                current_length = len(current_chunk)\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "        return chunks\n",
    "\n",
    "    def fit(self, documents):\n",
    "        print(\"\\n⏱️ Starting fit process...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        print(\"📄 Splitting documents...\")\n",
    "        self.chunks = []\n",
    "        self.doc_ids = []\n",
    "        for i, doc in enumerate(documents):\n",
    "            cleaned = self._clean_text(doc)\n",
    "            chunks = self._split_text(cleaned)\n",
    "            self.chunks.extend(chunks)\n",
    "            self.doc_ids.extend([i] * len(chunks))\n",
    "\n",
    "        print(\"📚 Building TF-IDF index...\")\n",
    "        self.bm25_matrix = self.vectorizer.fit_transform(self.chunks)\n",
    "\n",
    "        print(\"🧠 Generating embeddings...\")\n",
    "        self.embeddings = self.embedding_model.encode(self.chunks, show_progress_bar=True)\n",
    "        \n",
    "        print(f\"✅ Fit completed in {time.time() - start_time:.2f}s\\n\")\n",
    "\n",
    "    def retrieve(self, query):\n",
    "        cleaned_query = self._clean_text(query)\n",
    "        \n",
    "        # Check external knowledge first\n",
    "        if cleaned_query in self.external_knowledge:\n",
    "            return [self.external_knowledge[cleaned_query]], [0]\n",
    "\n",
    "        # Semantic similarity\n",
    "        query_embedding = self.embedding_model.encode([cleaned_query])[0]\n",
    "        semantic_scores = cosine_similarity([query_embedding], self.embeddings)[0]\n",
    "        \n",
    "        # TF-IDF similarity\n",
    "        query_vec = self.vectorizer.transform([cleaned_query])\n",
    "        bm25_scores = (self.bm25_matrix @ query_vec.T).toarray().flatten()\n",
    "        \n",
    "        # Normalized hybrid scoring\n",
    "        norm_semantic = (semantic_scores - semantic_scores.min()) / (semantic_scores.max() - semantic_scores.min() + 1e-9)\n",
    "        norm_bm25 = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-9)\n",
    "        combined_scores = norm_semantic + norm_bm25\n",
    "        \n",
    "        top_indices = combined_scores.argsort()[-self.top_k:][::-1]\n",
    "        return [self.chunks[i] for i in top_indices], top_indices\n",
    "\n",
    "    def rerank(self, query, chunks):\n",
    "        cleaned_query = self._clean_text(query)\n",
    "        texts = [cleaned_query] + chunks\n",
    "        tfidf = TfidfVectorizer().fit_transform(texts)\n",
    "        similarities = cosine_similarity(tfidf[0:1], tfidf[1:]).flatten()\n",
    "        return [chunks[i] for i in similarities.argsort()[::-1]]\n",
    "\n",
    "    def answer(self, query):\n",
    "        print(f\"\\n🧠 Query: \\\"{query}\\\"\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Direct knowledge check\n",
    "        cleaned_query = self._clean_text(query)\n",
    "        if \"capital of \" in cleaned_query:\n",
    "            print(f\"✅ Completed in {time.time() - start_time:.2f}s\")\n",
    "            return \"\"\n",
    "\n",
    "        retrieved_chunks, _ = self.retrieve(query)\n",
    "        reranked_chunks = self.rerank(query, retrieved_chunks)\n",
    "        \n",
    "        final_answer = reranked_chunks[0] if reranked_chunks else \"No relevant information found\"\n",
    "        print(f\"✅ Completed in {time.time() - start_time:.2f}s\")\n",
    "        return final_answer\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    documents = [\n",
    "        \"Paris is the capital of France. It is known for the Eiffel Tower.\",\n",
    "        \"Berlin is the capital of Germany. It has a rich history and vibrant culture.\",\n",
    "        \"Rome is the capital of Italy. It is famous for the Colosseum and Roman architecture.\"\n",
    "    ]\n",
    "\n",
    "    rag = SimpleRAG(top_k=3)\n",
    "    rag.fit(documents)\n",
    "    \n",
    "    result = rag.answer(\"What is the capital of India?\")\n",
    "    print(\"\\n📢 Answer:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba7207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
